<html>
<head>
<title>Majid Janzamin - Publications</title>
<style type="text/css">
body {
	font-family: verdana,arial,sans-serif;
	font-size: 10pt;
	margin: 30px;
	background-color: #EEFAF1;
	}
.auto-style1 {
	color: #0000FF;
}
.auto-style2 {
	margin-top: 0px;
}
.auto-style5 {
	color: #000080;
	font-size: xx-large;
}
.auto-style4 {
	text-align: center;
}
.auto-style6 {
	margin-top: 0px;
	color: #808080;
}
body,td,th {
	font-family: Arial, Helvetica, sans-serif;
	font-size: 12pt;
}
</style>
<base target="_self">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>

<h1>
<table style="width: 107%">
	<tr>
		<td class="auto-style5" style="width: 662px"><a href="index.html">Majid Janzamin</a></td>
		<td class="auto-style4">
		<img alt="UCI Wordmark" height="88" src="univ_calif_07_vt_289.jpg" width="140"></td>
	</tr>
</table>
<hr class="auto-style6" noshade="noshade" style="position: relative; left: 0px; top: 12px">
<hr class="auto-style6" noshade="noshade" style="position: relative; left: 0px; top: 5px">

<h2 class="auto-style1" style="height: 15px">Publications</h2>
<hr class="auto-style2" noshade="noshade" style="height: 5; position: relative; left: 0px; top: 0px">
<table style="width: 100%">
	<tr>
	  <td><p><strong>Note</strong>: authorship order of specified publications by <em><strong>asterisk symbol</strong></em> * follows the convention in theoretical computer science of <em><strong>alphabetical author order</strong></em>.</p>
		  <p>&nbsp;</p>
		  <p><strong>Preprints</strong>:</p>
		  <ul>
		    <li><a href="pubs/NN_GeneralizationBound.pdf">Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods</a><br>
		      By <strong>M. Janzamin</strong>, H. Sedghi, and A. Anandkumar<br>
		      Arxiv:1506.08473, June 2015. Submitted		  </li>
	    </ul>
		  <ul>
		    <li><a href="pubs/Features_DiscLearning.pdf">Score Function Features for Discriminative Learning: Matrix and Tensor Framework</a><br>
	        <strong>M. Janzamin</strong>, H. Sedghi, and A. Anandkumar<br>
	        Arxiv:1412.2863, Dec. 2014. In preparation		  </li>
	    </ul>
		  <ul>
		    <li><a href="pubs/AltTensorDecomp_Part2LVMs.pdf">Sample Complexity Analysis for Learning Overcomplete  Latent Variable Models through Tensor Methods</a><br>
	        A. Anandkumar, R. Ge, and <strong>M. Janzamin*</strong><br>
	        Arxiv:1408.0553, Aug. 2014., <a href="pubs/OvercompleteDecomposition-slides.pdf">Slides.</a><br>
		    </li>
	    </ul>
		  <ul>
		    <li><a href="pubs/AltTensorDecomp_Part1Algorithm.pdf">Guaranteed Non-Orthogonal Tensor Decomposition via Alternating Rank-1 Updates</a><br>
	        A. Anandkumar, R. Ge, and <strong>M. Janzamin*<br>
	        </strong>Arxiv:1402:5180, Feb. 2014. <a href="pubs/OvercompleteDecomposition-slides.pdf">Slides</a>.	<a href="../pubs/AltTensorDecompCodes.zip">Codes</a>. </li>
        </ul>
	    <blockquote>&nbsp;	    </blockquote>
	    <p><strong>Journal Publications</strong>:</p>
	    <ul>
	      <li><a href="pubs/powerdynamics.pdf">Analyzing Tensor Power Method Dynamics in Overcomplete Regime</a><br>
          A. Anandkumar, R. Ge, and <strong>M. Janzamin*</strong><br>
          Accepted for publication in<em> Journal of Machine Learning Research (JMLR)</em>. 2016.	    </li>
	    </ul>
	    <ul>
	      <li><a href="http://jmlr.org/papers/volume16/anandkumar15a/anandkumar15a.pdf">When Are Overcomplete Topic Models Identifiable? Uniqueness of  Tensor Tucker Decompositions with Structured Sparsity</a><br>
          A. Anandkumar, D. Hsu, <strong>M. Janzamin*</strong> and S. Kakade<br>
          In <em> Journal of Machine Learning Research (JMLR)</em>. Volume
          16:2643-2694, Dec. 2015.	    </li>
	    </ul>
	    <ul>
	      <li><a href="http://jmlr.csail.mit.edu/papers/volume15/janzamin14a/janzamin14a.pdf">High-Dimensional Covariance 
	        Decomposition into Sparse Markov and Independence Models</a><br>
	        <strong>M. Janzamin</strong> and A. Anandkumar<br>
	        In	        
	        <em> Journal of Machine Learning Research (JMLR)</em>. Volume
	        15:1549-1591, April 2014. <a href="pubs/JanzaminAnandkumar12-slides.pdf">Slides</a>.</li>
        </ul>
	    <p>&nbsp;</p>
	    <p><strong>Conference Publications</strong>:</p>
	    <ul>
	      <li><a href="http://arxiv.org/pdf/1412.3046v4.pdf">Provable Tensor Methods for Learning Mixtures of Generalized Linear Models</a><br>
          H. Sedghi, <strong>M. Janzamin</strong>,  and A. Anandkumar<br>
          In<em> Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</em>, Cadiz, Spain, May 2016.	    </li>
	    </ul>
	    <ul>
	      <li><a href="http://jmlr.csail.mit.edu/proceedings/papers/v44/janzamin2015.pdf">Feast at Play: Feature ExtrAction using Score function Tensors</a><br>
          <strong>M. Janzamin</strong>, H. Sedghi, U.N. Niranjan, and A. Anandkumar<br>          
          In<em> NIPS Feature Extraction: Modern Questions and Challenges</em>, Montreal, Canada, December 2015.	    </li>
	    </ul>
	    <ul>
	      <li><a href="pubs/AltTensorDecomp-COLT2015.pdf">Learning Overcomplete Latent Variable Models through Tensor Methods</a><br>
	        A. Anandkumar, R. Ge, and <strong>M. Janzamin*</strong><br>          
          In<em> Proceedings of the Conference on Learning Theory (COLT), Paris, France, July 2015</em>.	    </li>
	    </ul>
	    <ul>
	      <li><a href="pubs/Overcomplete_Ident_NIPS.pdf">When Are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</a><br>
          A. Anandkumar, D. Hsu, <strong>M. Janzamin*</strong> and S. Kakade<br>          
          In<em> Proceedings of theNeural Information Processing Systems (NIPS) Conference, Lake Tahoe, Nevada, United States, Dec 2013</em>.	    </li>
	    </ul>
	    <ul>
	      <li><a href="pubs/JanzaminAnandkumarICML12.pdf">High-Dimensional Covariance 
	        Decomposition into Sparse Markov and Independence Domains</a><br>
	        <strong>M. Janzamin</strong> and A. Anandkumar<br>	        
	        In<em> Proceedings of the 29th International Conference on Machine Learning (ICML)</em>, Edinburgh, Scotland, June 2012.	    </li>
	    </ul>
	    <ul>
	      <li><a href="pubs/WCNC2010.pdf">A Game-Theoretic Approach for Power 
	        Allocation in Bidirectional Cooperative Communication</a><br>
	        <strong>M. Janzamin</strong>, M. R. Pakravan, and H. Sedghi<br>	        
	        In<em> Proceedings of IEEE Wireless Communications and Networking Conference (WCNC)</em>, Sydney, Australia, April 2010.	    </li>
	    </ul>
	    <p>&nbsp;</p>
	    <div title="Page 3">
	      <div>
	        <div>
	          <p><strong>Workshop/Conference Presentations: </strong></p>
	          <ul>
	            <li>
	              <p>Generalization Bounds for Neural Networks through Tensor Factorization. ISMP (International Symposium on Optimization), Pittsburgh, PA, July 2015. </p>
                </li>
	            <li>
	              <p>Score Function Features for Discriminative Learning. ICLR (International Conference on Learning Representations), San Diego, CA, May 2015. </p>
                </li>
	            <li>
	              <p>Provable Learning of Overcomplete Latent Variable Models: Semi-supervised and Unsupervised Settings. NIPS: Workshop on Optimization for Machine Learning, Montreal, Canada, Dec. 2014. </p>
                </li>
	            <li>
	              <p>Provable Learning of Overcomplete Latent Variable Models: Semi-supervised and Unsupervised Settings. New England Machine Learning Day, Microsoft Research, Cambridge, MA, May 2014. </p>
                </li>
	            <li>
	              <p>Guaranteed Non-Orthogonal Tensor Decomposition through Alternating Rank-1 Updates. Workshop on Electrical Flows, Graph Laplacians, and Algorithms: Spectral Graph Theory and Beyond, ICERM, Brown University, Providence, RI, April 2014. </p>
                </li>
	            <li>
	              <p>When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity. GlobalSIP: Symposium on Optimization in Machine Learning and Signal Processing, Austin, TX, Dec. 2013. </p>
                </li>
	            <li>
	              <p>High-Dimensional Covariance Decomposition into Sparse Markov and Independence Domains. 50th Annual Allerton Conference on Communication, Control, and Computing, Monticello, IL, Oct. 2012 </p>
                </li>
              </ul>
            </div>
          </div>
        </div>
<p>&nbsp;</p>
      </td>
	</tr>
	<tr>
	  <td>&nbsp;</td>
  </tr>
	<tr>
	  <td>&nbsp;</td>
  </tr>
</table>

</body>
</html>