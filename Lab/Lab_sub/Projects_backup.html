<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<title>MEGA DatA Lab</title>
<style type="text/css">
.class1 {
	color: #093;
}
.class2 {
	color: #00F;
}
.class2 font {
	color: #009;
}
</style>

<!-- style type="text/css">

/* All Styles Optional */

* {
font-family:calibri,arial;
font-size:16pt;
}

div#show {
background-color:#efefe7;
width:400px;
margin:0; padding:2px;
border:1px solid #909090;
}

div#show table input,  div#show4 table input {
outline-style:none;
}

</style -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript"
  src="https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script src="swissarmy.js" type="text/javascript"></script>

<script src="slideshow.js" type="text/javascript"></script>

</head>

<body>
<h1 align="center"><font color="#000080">MEGA DatA Lab</font></h1>
<h2 align="center"><span class="class1"><span class="class"> Model Estimation, Graphical Algorithms, Data Analysis Lab</span></span></h2>
<p align="center">EECS, University of California, Irvine, CA 92697.</p>
<hr />
<div id="show"><script type="text/javascript">new inter_slide(slideShow)</script></div>
<hr />
<h3 align="center" class="class2"> <a href="../Lab.html">Home</a> | <a href="Group members.html">Group members</a> | <a href="../../research.html">Publications</a> | Projects | <a href="Announcements.html">Announcements</a></h3>
<hr />

<p align="left"></p>
<h2 align="left" class="class2"><font color="#990099">Parameter Estimation in Latent Variable Models</font></h2>
<h3 align="left"><font color="#990099">Fast Detection of Overlapping Communities via Online Tensor Methods</font></h3>
<p align="left">We present a fast tensor-based approach for detecting hidden overlapping communities under the Mixed Membership Stochastic Blockmodel (MMSB). We present two implementations, viz., a GPU-based implementation which exploits the parallelism of SIMD architectures and a CPU-based implementation for larger datasets, wherein the GPU memory does not suffice. Our GPU-based implementation involves a careful optimization of storage, data transfer and matrix computations. Our CPU-based implementation involves sparse linear algebraic operations which exploit the data sparsity. We use stochastic gradient descent for multilinear spectral optimization and this allows for flexibility in the tradeoff between node sub-sampling and accuracy of the results. We validate our results on datasets from Facebook, Yelp and DBLP where ground truth is available, using notions of $p$-values and false discovery rates, and obtain high accuracy for membership recovery. We compare our results, both in terms of execution time and accuracy, to the state-of-the-art algorithms such as the variational method, and report many orders of magnitude gain in the execution time. For instance, for the DBLP dataset with about a million nodes and 16 million edges, the execution time is about two minutes. </p>
<p align="left">&nbsp;</p>
<hr />


<h2 align="left" class="class2"><font color="#990099">Structure Estimation in Graphical Models</font></h2>
<h3 align="left"><font color="#990099">Markov + Independence Model</font></h3>
<p align="left">Fitting high-dimensional data involves   a delicate tradeoff between faithful representation and the use of sparse models. Too often, sparsity assumptions on the fitted model are too restrictive to provide a faithful representation of the observed data. In this project, we present a novel framework incorporating  sparsity in different domains.<img src="Projects_sub/MarkovIndepModel.png" alt="MarkovIndepModel" width="300" height="101" align="right" /></p>
<p align="left">We decompose the observed covariance matrix into a sparse Gaussian Markov model (with a sparse precision matrix)  and a sparse independence model (with a sparse covariance matrix). This is shown in the following model. Our framework incorporates sparse covariance and sparse precision estimation as special cases and thus introduces a richer class of high-dimensional models.</p>
<p align="left">We  characterize   sufficient conditions for identifiability  of the two models, viz.,  Markov and   independence models. We propose an efficient decomposition method based on a  modification of the popular $\ell_1$-penalized maximum-likelihood estimator ($\ell_1$-MLE). We establish that our estimator is consistent in both the domains, i.e., it successfully recovers the supports of both   Markov and   independence models, where the number of samples could be much more than the number of variables (high dimensional regime). Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation. The proposed algorithm was also applied to the &quot;foreign exchange rate&quot; and &quot;stock market returns&quot; data sets. Detailed results can be found in the following paper.</p>
<p align="left"><b>&#8220;High-Dimensional Covariance Decomposition into Sparse Markov
  and Independence Domains.  &#8221;</b> by M. Janzamin and A. Anandkumar. <em>Preprint, Feb. 2012.</em> <em>An abridged version appears in the Proc. of ICML, June 2012</em><a href="pubs/ChoiJMLR10-abstract.html"></a>.<br />
Download: <a href="../../pubs/JanzaminAnandkumar12.pdf">PDF.</a> <a href="../../pubs/JanzaminAnandkumarICML12.pdf">ICML-version.</a> <a href="../../pubs/JanzaminAnandkumar12-slides.pdf">Slides.</a></p>
<p align="left">&nbsp;</p>
<!-- h2 align="left" class="class2"><font color="#990099">Structure Estimation in High-Dimensional Graphical Models</font></h2 -->
<h3 align="left"><font color="#990099"><strong>Gaussian Models: Walk-Summability and Local Separation Critetrion</strong></font></h3>
<p>Fitting high-dimensional data involves   a delicate tradeoff between faithful representation and the use of sparse models. Too often, sparsity assumptions on the fitted model are too restrictive to provide a faithful representation of the observed data. In this project, we present a novel framework incorporating  sparsity in different domains.</p>
<p>We consider the problem of high-dimensional Gaussian graphical model selection. We identify a set of graphs for which an efficient estimation algorithm exits, and this algorithm is based on thresholding of empirical conditional covariances. Under a set of transparent conditions, we establish structural consistency (or sparsistency) for the proposed algorithm, when the number of samples $n=\Omega(J_{\min}^{-2} \log p)$, where $p$ is the number of variables and $J_{\min}$ is the minimum (absolute) edge potential of the graphical model. The sufficient conditions for sparsistency are based on the notion of walk-summability of the model and the presence of sparse local vertex separators in the underlying graph. We also derive novel non-asymptotic necessary conditions on the number of samples required for sparsistency.</p>
<p><b>&#8220;<strong>High-Dimensional Gaussian Graphical Model Selection: Walk-Summability and Local Separation Criterion</strong>&#8221;</b> by A. Anandkumar, V.Y.F Tan, F. Huang, and A.S. Willsky.&nbsp;<em>J. Machine Learning Research, 13:2293&ndash;2337, Aug. 2012. An abridged version appears in the Proc. of NIPS, Dec. 2011</em>.&nbsp;<br />
<a href="http://newport.eecs.uci.edu/anandkumar/pubs/AnandkumarTanGaussian11-abstract.html">Details.</a>&nbsp;Download:&nbsp;<a href="http://newport.eecs.uci.edu/anandkumar/pubs/AnandkumarTan-Gaussian11.pdf">PDF</a>.&nbsp;<a href="http://newport.eecs.uci.edu/anandkumar/pubs/GaussianStructLearning-code.zip">Softwa</a><a href="http://newport.eecs.uci.edu/anandkumar/pubs/GaussianStructLearning-code.zip">re code and datasets.</a>&nbsp;<a href="http://newport.eecs.uci.edu/anandkumar/pubs/AnandkumarTan11-shortver.pdf">Short (NIPS) Version</a>&nbsp;<a href="http://videolectures.net/nips2011_anandkumar_conditions/">Videolecture</a>&nbsp;<a href="http://newport.eecs.uci.edu/anandkumar/pubs/AnandkumarNIPS11-slides.pdf">NIPS-Slides</a></p>
<p>&nbsp;</p>
<h3><font color="#990099"><strong>Ising Models: Local Separation Criterion </strong></font></h3>
<p>We consider the problem of high-dimensional Ising (graphical) model selection. We propose a simple algorithm for structure estimation based on the thresholding of the empirical conditional variation distances. We introduce a novel criterion for tractable graph families, where this method is efficient, based on the presence of sparse local separators between node pairs in the underlying graph. For such graphs, the proposed algorithm has a sample complexity of  $n=\Omega(J_{\min}^{-2} \log p)$, where $p$ is the number of variables and $J_{\min}$ is the minimum (absolute) edge potential of the graphical model. We also establish nonasymptotic necessary and sufficient conditions for structure estimation.</p>
<p><strong>&ldquo;High-Dimensional Structure Learning of Ising Models: Local Separation Criterion&rdquo;</strong>&nbsp;by A. Anandkumar, V.Y.F Tan, F. Huang, and A.S. Willsky.&nbsp;<em>Annals of Statistics, Volume 40, Number 3 (2012), 1346-1375.&nbsp;</em><em>An abridged version appears in the Proc. of NIPS, Dec. 2011</em>.<br />
<a href="http://newport.eecs.uci.edu/anandkumar/pubs/AnandkumarTanIsing11-abstract.html">Details.</a>&nbsp;Download:&nbsp;<a href="http://newport.eecs.uci.edu/anandkumar/pubs/AnandkumarTan-Ising11.pdf">PDF</a>.&nbsp;<a href="http://newport.eecs.uci.edu/anandkumar/pubs/AnandkumarTan-Ising11-supp.pdf">Supplementary file.</a>&nbsp;<a href="http://newport.eecs.uci.edu/anandkumar/pubs/code_IsingModel.zip">Software code and datasets.</a>&nbsp;<a href="http://newport.eecs.uci.edu/anandkumar/pubs/AnandkumarTan11-shortver.pdf">Short (NIPS) Version</a>&nbsp;<a href="http://videolectures.net/nips2011_anandkumar_conditions/">Videolecture</a>&nbsp;<a href="http://newport.eecs.uci.edu/anandkumar/pubs/AnandkumarNIPS11-slides.pdf">NIPS-Slides</a></p>
<h3><br />
</h3>
<h3><strong><font color="#990099">Mixtures of Graphical Models</font></strong></h3>
<p>We consider unsupervised estimation of mixtures of discrete graphical models, where the class variable corresponding to the mixture components is hidden and each mixture component over the observed variables can have a potentially different Markov graph structure and parameters. We propose a novel moment-based approach for estimating the mixture components, and our output is a tree-mixture model which serves as a good approximation to the underlying graphical model mixture. Our method is efficient when the union graph, which is the union of the Markov graphs of the mixture components with sparse vertex separators between any pair of observed variables. This includes tree mixtures and mixtures of bounded degree graphs. For such models, we prove that our method correctly recovers the union graph structure and the tree structures corresponding to maximumlikelihood tree approximations of the mixture components. The sample and computational complexities of our method scale as poly$(p, r)$, for an $r$-component mixture of $p$-variate graphical models. Our approach offers a powerful alternative to heuristics such as expectation maximization (EM) for learning mixture models.</p>
<p><strong>&ldquo;Learning High-Dimensional Mixtures of Graphical Models &rdquo;</strong>&nbsp;by A. Anandkumar, D. Hsu, F. Huang, and S.M. Kakade.&nbsp;<em>An abridged version appears in NIPS 2012.&nbsp;</em><br />
Download:&nbsp;<a href="http://newport.eecs.uci.edu/anandkumar/pubs/AnandkumarEtal_graphicalmixtures12.pdf">PDF.</a>&nbsp;<a href="http://newport.eecs.uci.edu/anandkumar/pubs/AnandkumarLIDS12.pdf">Slides.</a>&nbsp;<a href="http://newport.eecs.uci.edu/anandkumar/pubs/AnandkumarHsuHuangKakadeNIPS12.pdf">NIPS-version</a></p>
<p align="left">&nbsp;</p>

<hr />

<p align="left"></p>
<p align="left">&nbsp;</p>

</body>
</html>
